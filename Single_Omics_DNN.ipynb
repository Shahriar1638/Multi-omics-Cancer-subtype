{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2d18ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from lifelines.utils import concordance_index\n",
    "from mofapy2.run.entry_point import entry_point\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "from torch_geometric.nn import GATConv, GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import knn_graph\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d60258a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=64):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, latent_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, input_dim),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon, z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f744738",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.classifier = nn.Linear(128, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embeddings = self.embedding(x)\n",
    "        output = self.classifier(embeddings)\n",
    "        return output\n",
    "    \n",
    "    def get_embeddings(self, x):\n",
    "        \"\"\"Extract embeddings from the hidden layer\"\"\"\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52e9512e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading preprocessed datasets...\n"
     ]
    }
   ],
   "source": [
    "print(\"üîÑ Loading preprocessed datasets...\")\n",
    "expression_scaled2 = pd.read_csv(\"../Updated_model_nd_dataset/processed_expression_FXS_OG.csv\", index_col=0)\n",
    "methylation_scaled2 = pd.read_csv(\"../Updated_model_nd_dataset/processed_methylation_FXS_OG.csv\", index_col=0)\n",
    "copy_number_scaled2 = pd.read_csv(\"../Updated_model_nd_dataset/processed_cnv_FXS_OG.csv\", index_col=0)\n",
    "phenotype_data_clean2 = pd.read_csv(\"../Updated_model_nd_dataset/processed_phenotype_FXS_OG.csv\", index_col=0)\n",
    "labels = pd.read_csv(\"../Updated_model_nd_dataset/processed_labels_3Omics_FXS_OG.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac4cfce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIXED INTEGRATION SYSTEM - HIERARCHICAL MULTI-MODAL FUSION\n",
      "================================================================================\n",
      "üîÑ Preparing individual omics datasets...\n",
      "üìä Individual Dataset Shapes:\n",
      "   Expression: (205, 60660)\n",
      "   Methylation: (205, 220147)\n",
      "   Copy Number: (205, 56756)\n",
      "   Phenotype: (205, 78)\n",
      "   Labels: (205, 1)\n",
      "\n",
      "‚úÖ Sample alignment: Perfect\n",
      "\n",
      "üîç Data Quality Check:\n",
      "   Expression missing: 0\n",
      "   Methylation missing: 0\n",
      "   CNV missing: 0\n",
      "   Phenotype missing: 3318\n",
      "\n",
      "üìã Ready for Mixed Integration Analysis:\n",
      "   Total samples: 205\n",
      "   Number of modalities: 4\n",
      "   Number of classes: subtype_encoded    4\n",
      "dtype: int64\n",
      "\n",
      "‚úÖ Data preparation completed for hierarchical fusion!\n"
     ]
    }
   ],
   "source": [
    "# Mixed Integration System - Hierarchical Multi-Modal Fusion\n",
    "print(\"MIXED INTEGRATION SYSTEM - HIERARCHICAL MULTI-MODAL FUSION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Transpose data for ML models (samples as rows)\n",
    "print(\"üîÑ Preparing individual omics datasets...\")\n",
    "expression_data = expression_scaled2.T  # (samples x genes)\n",
    "methylation_data = methylation_scaled2.T  # (samples x probes)\n",
    "cnv_data = copy_number_scaled2.T  # (samples x regions)\n",
    "phenotype_data = phenotype_data_clean2  # (samples x clinical features)\n",
    "\n",
    "print(\"üìä Individual Dataset Shapes:\")\n",
    "print(f\"   Expression: {expression_data.shape}\")\n",
    "print(f\"   Methylation: {methylation_data.shape}\")\n",
    "print(f\"   Copy Number: {cnv_data.shape}\")\n",
    "print(f\"   Phenotype: {phenotype_data.shape}\")\n",
    "print(f\"   Labels: {labels.shape}\")\n",
    "\n",
    "# Verify sample alignment\n",
    "sample_sets = [\n",
    "    set(expression_data.index),\n",
    "    set(methylation_data.index),\n",
    "    set(cnv_data.index),\n",
    "    set(phenotype_data.index),\n",
    "    set(labels.index)\n",
    "]\n",
    "\n",
    "all_aligned = all(s == sample_sets[0] for s in sample_sets)\n",
    "print(f\"\\n‚úÖ Sample alignment: {'Perfect' if all_aligned else 'Misaligned'}\")\n",
    "\n",
    "# Data quality check\n",
    "print(f\"\\nüîç Data Quality Check:\")\n",
    "print(f\"   Expression missing: {expression_data.isnull().sum().sum()}\")\n",
    "print(f\"   Methylation missing: {methylation_data.isnull().sum().sum()}\")\n",
    "print(f\"   CNV missing: {cnv_data.isnull().sum().sum()}\")\n",
    "print(f\"   Phenotype missing: {phenotype_data.isnull().sum().sum()}\")\n",
    "\n",
    "# Prepare individual datasets\n",
    "omics_datasets = {\n",
    "    'expression': expression_data.values,\n",
    "    'methylation': methylation_data.values,\n",
    "    'cnv': cnv_data.values,\n",
    "    'phenotype': phenotype_data.values\n",
    "}\n",
    "\n",
    "print(f\"\\nüìã Ready for Mixed Integration Analysis:\")\n",
    "print(f\"   Total samples: {len(expression_data)}\")\n",
    "print(f\"   Number of modalities: {len(omics_datasets)}\")\n",
    "print(f\"   Number of classes: {labels.nunique()}\")\n",
    "\n",
    "print(\"\\n‚úÖ Data preparation completed for hierarchical fusion!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1ffe794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining Mixed Integration Architecture...\n",
      "==================================================\n",
      "‚úÖ Mixed Integration Architecture defined!\n",
      "üèóÔ∏è Components:\n",
      "   - ModalitySpecificAutoencoder: Individual omics encoders\n",
      "   - HierarchicalFusionAutoencoder: Second-stage fusion\n",
      "   - MixedIntegrationSystem: Complete end-to-end system\n"
     ]
    }
   ],
   "source": [
    "# Mixed Integration Architecture Classes\n",
    "print(\"Defining Mixed Integration Architecture...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "class ModalitySpecificAutoencoder(nn.Module):\n",
    "    \"\"\"Autoencoder for individual omics modalities\"\"\"\n",
    "    def __init__(self, input_dim, latent_dim=64, hidden_dim=256):\n",
    "        super(ModalitySpecificAutoencoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, latent_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon, z\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"Get only the latent representation\"\"\"\n",
    "        return self.encoder(x)\n",
    "\n",
    "class ModalitySpecificMLP(nn.Module):\n",
    "    \"\"\"MLP for individual omics modalities\"\"\"\n",
    "    def __init__(self, input_dim, embedding_dim=64, hidden_dim=256):\n",
    "        super(ModalitySpecificMLP, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, embedding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.feature_extractor(x)\n",
    "\n",
    "class HierarchicalFusionAutoencoder(nn.Module):\n",
    "    \"\"\"Second-stage autoencoder for fusing modality-specific embeddings\"\"\"\n",
    "    def __init__(self, total_embedding_dim, fusion_latent_dim=32, hidden_dim=128):\n",
    "        super(HierarchicalFusionAutoencoder, self).__init__()\n",
    "        self.total_embedding_dim = total_embedding_dim\n",
    "        self.fusion_latent_dim = fusion_latent_dim\n",
    "        \n",
    "        # Fusion encoder\n",
    "        self.fusion_encoder = nn.Sequential(\n",
    "            nn.Linear(total_embedding_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, fusion_latent_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Fusion decoder\n",
    "        self.fusion_decoder = nn.Sequential(\n",
    "            nn.Linear(fusion_latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, total_embedding_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z_fusion = self.fusion_encoder(x)\n",
    "        x_recon = self.fusion_decoder(z_fusion)\n",
    "        return x_recon, z_fusion\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"Get only the fusion representation\"\"\"\n",
    "        return self.fusion_encoder(x)\n",
    "\n",
    "class HierarchicalFusionMLP(nn.Module):\n",
    "    \"\"\"Second-stage MLP for fusing modality-specific embeddings\"\"\"\n",
    "    def __init__(self, total_embedding_dim, num_classes, fusion_dim=64, hidden_dim=128):\n",
    "        super(HierarchicalFusionMLP, self).__init__()\n",
    "        \n",
    "        # Fusion layer\n",
    "        self.fusion_layer = nn.Sequential(\n",
    "            nn.Linear(total_embedding_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, fusion_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Linear(fusion_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        fusion_features = self.fusion_layer(x)\n",
    "        output = self.classifier(fusion_features)\n",
    "        return output\n",
    "    \n",
    "    def get_fusion_features(self, x):\n",
    "        \"\"\"Get fusion layer features\"\"\"\n",
    "        return self.fusion_layer(x)\n",
    "\n",
    "class MixedIntegrationSystem(nn.Module):\n",
    "    \"\"\"Complete Mixed Integration System\"\"\"\n",
    "    def __init__(self, modality_dims, modality_latent_dims, fusion_latent_dim, num_classes):\n",
    "        super(MixedIntegrationSystem, self).__init__()\n",
    "        self.modality_names = list(modality_dims.keys())\n",
    "        self.modality_dims = modality_dims\n",
    "        self.modality_latent_dims = modality_latent_dims\n",
    "        \n",
    "        # Stage 1: Modality-specific autoencoders\n",
    "        self.modality_autoencoders = nn.ModuleDict()\n",
    "        for modality, input_dim in modality_dims.items():\n",
    "            self.modality_autoencoders[modality] = ModalitySpecificAutoencoder(\n",
    "                input_dim, modality_latent_dims[modality]\n",
    "            )\n",
    "        \n",
    "        # Stage 2: Fusion autoencoder\n",
    "        total_embedding_dim = sum(modality_latent_dims.values())\n",
    "        self.fusion_autoencoder = HierarchicalFusionAutoencoder(\n",
    "            total_embedding_dim, fusion_latent_dim\n",
    "        )\n",
    "        \n",
    "        # Final classifier\n",
    "        self.classifier = nn.Linear(fusion_latent_dim, num_classes)\n",
    "    \n",
    "    def forward(self, modality_inputs):\n",
    "        # Stage 1: Extract modality-specific embeddings\n",
    "        modality_embeddings = []\n",
    "        reconstruction_losses = []\n",
    "        \n",
    "        for modality in self.modality_names:\n",
    "            x = modality_inputs[modality]\n",
    "            x_recon, z = self.modality_autoencoders[modality](x)\n",
    "            modality_embeddings.append(z)\n",
    "            reconstruction_losses.append(F.mse_loss(x_recon, x))\n",
    "        \n",
    "        # Concatenate all modality embeddings\n",
    "        concatenated_embeddings = torch.cat(modality_embeddings, dim=1)\n",
    "        \n",
    "        # Stage 2: Fusion\n",
    "        fusion_recon, fusion_embedding = self.fusion_autoencoder(concatenated_embeddings)\n",
    "        fusion_loss = F.mse_loss(fusion_recon, concatenated_embeddings)\n",
    "        \n",
    "        # Classification\n",
    "        class_output = self.classifier(fusion_embedding)\n",
    "        \n",
    "        return {\n",
    "            'class_output': class_output,\n",
    "            'fusion_embedding': fusion_embedding,\n",
    "            'modality_embeddings': modality_embeddings,\n",
    "            'reconstruction_losses': reconstruction_losses,\n",
    "            'fusion_loss': fusion_loss\n",
    "        }\n",
    "    \n",
    "    def get_embeddings(self, modality_inputs):\n",
    "        \"\"\"Extract embeddings without gradients\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            return self.forward(modality_inputs)\n",
    "\n",
    "print(\"‚úÖ Mixed Integration Architecture defined!\")\n",
    "print(\"üèóÔ∏è Components:\")\n",
    "print(\"   - ModalitySpecificAutoencoder: Individual omics encoders\")\n",
    "print(\"   - HierarchicalFusionAutoencoder: Second-stage fusion\")\n",
    "print(\"   - MixedIntegrationSystem: Complete end-to-end system\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94e7cc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining Mixed Integration Training Functions...\n",
      "==================================================\n",
      "‚úÖ Mixed Integration Training Functions defined!\n",
      "üîß Available training modes:\n",
      "   - End-to-end: train_mixed_integration_system()\n",
      "   - Stage-wise: train_stage_wise_mixed_integration()\n"
     ]
    }
   ],
   "source": [
    "# Mixed Integration Training Functions\n",
    "print(\"Defining Mixed Integration Training Functions...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def prepare_mixed_integration_data(omics_datasets, labels, test_size=0.2, device='cpu'):\n",
    "    \"\"\"Prepare data for mixed integration training\"\"\"\n",
    "    # Get common samples\n",
    "    n_samples = len(labels)\n",
    "    indices = np.arange(n_samples)\n",
    "    \n",
    "    # Split indices\n",
    "    train_idx, test_idx = train_test_split(\n",
    "        indices, test_size=test_size, random_state=42, \n",
    "        stratify=labels.values.ravel()\n",
    "    )\n",
    "    \n",
    "    # Split each modality\n",
    "    train_data = {}\n",
    "    test_data = {}\n",
    "    \n",
    "    for modality, data in omics_datasets.items():\n",
    "        train_data[modality] = torch.FloatTensor(data[train_idx]).to(device)\n",
    "        test_data[modality] = torch.FloatTensor(data[test_idx]).to(device)\n",
    "    \n",
    "    # Labels\n",
    "    y_train = torch.LongTensor(labels.values.ravel()[train_idx]).to(device)\n",
    "    y_test = torch.LongTensor(labels.values.ravel()[test_idx]).to(device)\n",
    "    \n",
    "    return {\n",
    "        'train_data': train_data,\n",
    "        'test_data': test_data,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "        'train_idx': train_idx,\n",
    "        'test_idx': test_idx\n",
    "    }\n",
    "\n",
    "def train_mixed_integration_system(data_dict, modality_dims, config, device):\n",
    "    \"\"\"Train the complete mixed integration system\"\"\"\n",
    "    print(f\"üöÄ Training Mixed Integration System...\")\n",
    "    print(f\"Configuration: {config}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = MixedIntegrationSystem(\n",
    "        modality_dims=modality_dims,\n",
    "        modality_latent_dims=config['modality_latent_dims'],\n",
    "        fusion_latent_dim=config['fusion_latent_dim'],\n",
    "        num_classes=config['num_classes']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Optimizers and criteria\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['lr'])\n",
    "    classification_criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [],\n",
    "        'recon_losses': [], 'fusion_losses': []\n",
    "    }\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(config['epochs']):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(data_dict['train_data'])\n",
    "        \n",
    "        # Classification loss\n",
    "        class_loss = classification_criterion(outputs['class_output'], data_dict['y_train'])\n",
    "        \n",
    "        # Reconstruction losses\n",
    "        total_recon_loss = sum(outputs['reconstruction_losses'])\n",
    "        fusion_loss = outputs['fusion_loss']\n",
    "        \n",
    "        # Total loss with weighting\n",
    "        total_loss = (class_loss + \n",
    "                     config['recon_weight'] * total_recon_loss + \n",
    "                     config['fusion_weight'] * fusion_loss)\n",
    "        \n",
    "        # Backward pass\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs['class_output'], 1)\n",
    "        accuracy = (predicted == data_dict['y_train']).float().mean().item()\n",
    "        \n",
    "        # Store history\n",
    "        history['train_loss'].append(total_loss.item())\n",
    "        history['train_acc'].append(accuracy)\n",
    "        history['recon_losses'].append(total_recon_loss.item())\n",
    "        history['fusion_losses'].append(fusion_loss.item())\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{config['epochs']}], \"\n",
    "                  f\"Total Loss: {total_loss.item():.4f}, \"\n",
    "                  f\"Class Loss: {class_loss.item():.4f}, \"\n",
    "                  f\"Recon Loss: {total_recon_loss.item():.4f}, \"\n",
    "                  f\"Fusion Loss: {fusion_loss.item():.4f}, \"\n",
    "                  f\"Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Training set evaluation\n",
    "        train_outputs = model(data_dict['train_data'])\n",
    "        _, train_pred = torch.max(train_outputs['class_output'], 1)\n",
    "        train_accuracy = (train_pred == data_dict['y_train']).float().mean().item()\n",
    "        \n",
    "        # Test set evaluation\n",
    "        test_outputs = model(data_dict['test_data'])\n",
    "        _, test_pred = torch.max(test_outputs['class_output'], 1)\n",
    "        test_accuracy = (test_pred == data_dict['y_test']).float().mean().item()\n",
    "        \n",
    "        # Extract embeddings\n",
    "        train_embeddings = train_outputs['fusion_embedding'].cpu().numpy()\n",
    "        test_embeddings = test_outputs['fusion_embedding'].cpu().numpy()\n",
    "        modality_train_embeddings = [emb.cpu().numpy() for emb in train_outputs['modality_embeddings']]\n",
    "        modality_test_embeddings = [emb.cpu().numpy() for emb in test_outputs['modality_embeddings']]\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'train_embeddings': train_embeddings,\n",
    "        'test_embeddings': test_embeddings,\n",
    "        'modality_train_embeddings': modality_train_embeddings,\n",
    "        'modality_test_embeddings': modality_test_embeddings,\n",
    "        'train_pred': train_pred.cpu().numpy(),\n",
    "        'test_pred': test_pred.cpu().numpy()\n",
    "    }\n",
    "\n",
    "def train_stage_wise_mixed_integration(data_dict, modality_dims, config, device):\n",
    "    \"\"\"Train mixed integration in two separate stages\"\"\"\n",
    "    print(f\"üéØ Training Stage-wise Mixed Integration...\")\n",
    "    \n",
    "    # Stage 1: Train individual modality autoencoders\n",
    "    print(\"üìç Stage 1: Training modality-specific autoencoders...\")\n",
    "    modality_models = {}\n",
    "    modality_embeddings_train = {}\n",
    "    modality_embeddings_test = {}\n",
    "    \n",
    "    for modality, input_dim in modality_dims.items():\n",
    "        print(f\"  Training {modality} autoencoder...\")\n",
    "        \n",
    "        # Initialize model\n",
    "        ae = ModalitySpecificAutoencoder(\n",
    "            input_dim, config['modality_latent_dims'][modality]\n",
    "        ).to(device)\n",
    "        \n",
    "        optimizer = optim.Adam(ae.parameters(), lr=config['lr'])\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        # Training\n",
    "        ae.train()\n",
    "        for epoch in range(config['stage1_epochs']):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            x_recon, z = ae(data_dict['train_data'][modality])\n",
    "            loss = criterion(x_recon, data_dict['train_data'][modality])\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (epoch + 1) % 50 == 0:\n",
    "                print(f\"    Epoch [{epoch+1}/{config['stage1_epochs']}], Loss: {loss.item():.6f}\")\n",
    "        \n",
    "        # Extract embeddings\n",
    "        ae.eval()\n",
    "        with torch.no_grad():\n",
    "            _, z_train = ae(data_dict['train_data'][modality])\n",
    "            _, z_test = ae(data_dict['test_data'][modality])\n",
    "            \n",
    "            modality_embeddings_train[modality] = z_train\n",
    "            modality_embeddings_test[modality] = z_test\n",
    "            modality_models[modality] = ae\n",
    "    \n",
    "    # Stage 2: Train fusion system\n",
    "    print(\"üìç Stage 2: Training fusion system...\")\n",
    "    \n",
    "    # Concatenate modality embeddings\n",
    "    train_concat = torch.cat(list(modality_embeddings_train.values()), dim=1)\n",
    "    test_concat = torch.cat(list(modality_embeddings_test.values()), dim=1)\n",
    "    \n",
    "    # Fusion autoencoder + classifier\n",
    "    total_embedding_dim = sum(config['modality_latent_dims'].values())\n",
    "    fusion_ae = HierarchicalFusionAutoencoder(\n",
    "        total_embedding_dim, config['fusion_latent_dim']\n",
    "    ).to(device)\n",
    "    \n",
    "    classifier = nn.Linear(config['fusion_latent_dim'], config['num_classes']).to(device)\n",
    "    \n",
    "    # Combined training\n",
    "    fusion_optimizer = optim.Adam(\n",
    "        list(fusion_ae.parameters()) + list(classifier.parameters()), \n",
    "        lr=config['lr']\n",
    "    )\n",
    "    \n",
    "    recon_criterion = nn.MSELoss()\n",
    "    class_criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    fusion_history = {'loss': [], 'acc': []}\n",
    "    \n",
    "    for epoch in range(config['stage2_epochs']):\n",
    "        fusion_optimizer.zero_grad()\n",
    "        \n",
    "        # Fusion autoencoder\n",
    "        fusion_recon, fusion_z = fusion_ae(train_concat)\n",
    "        recon_loss = recon_criterion(fusion_recon, train_concat)\n",
    "        \n",
    "        # Classification\n",
    "        class_output = classifier(fusion_z)\n",
    "        class_loss = class_criterion(class_output, data_dict['y_train'])\n",
    "        \n",
    "        # Combined loss\n",
    "        total_loss = recon_loss + class_loss\n",
    "        total_loss.backward()\n",
    "        fusion_optimizer.step()\n",
    "        \n",
    "        # Accuracy\n",
    "        _, predicted = torch.max(class_output, 1)\n",
    "        accuracy = (predicted == data_dict['y_train']).float().mean().item()\n",
    "        \n",
    "        fusion_history['loss'].append(total_loss.item())\n",
    "        fusion_history['acc'].append(accuracy)\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"  Epoch [{epoch+1}/{config['stage2_epochs']}], \"\n",
    "                  f\"Loss: {total_loss.item():.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Final evaluation\n",
    "    fusion_ae.eval()\n",
    "    classifier.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Training evaluation\n",
    "        _, train_fusion_z = fusion_ae(train_concat)\n",
    "        train_class_output = classifier(train_fusion_z)\n",
    "        _, train_pred = torch.max(train_class_output, 1)\n",
    "        train_accuracy = (train_pred == data_dict['y_train']).float().mean().item()\n",
    "        \n",
    "        # Test evaluation\n",
    "        _, test_fusion_z = fusion_ae(test_concat)\n",
    "        test_class_output = classifier(test_fusion_z)\n",
    "        _, test_pred = torch.max(test_class_output, 1)\n",
    "        test_accuracy = (test_pred == data_dict['y_test']).float().mean().item()\n",
    "    \n",
    "    return {\n",
    "        'modality_models': modality_models,\n",
    "        'fusion_autoencoder': fusion_ae,\n",
    "        'classifier': classifier,\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'train_embeddings': train_fusion_z.cpu().numpy(),\n",
    "        'test_embeddings': test_fusion_z.cpu().numpy(),\n",
    "        'modality_train_embeddings': {k: v.cpu().numpy() for k, v in modality_embeddings_train.items()},\n",
    "        'modality_test_embeddings': {k: v.cpu().numpy() for k, v in modality_embeddings_test.items()},\n",
    "        'fusion_history': fusion_history\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Mixed Integration Training Functions defined!\")\n",
    "print(\"üîß Available training modes:\")\n",
    "print(\"   - End-to-end: train_mixed_integration_system()\")\n",
    "print(\"   - Stage-wise: train_stage_wise_mixed_integration()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "905c932e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIXED INTEGRATION EXPERIMENTAL SETUP\n",
      "================================================================================\n",
      "üìä Experimental Configuration:\n",
      "   Number of classes: 4\n",
      "   Total samples: 205\n",
      "   Device: cuda\n",
      "\n",
      "üìã Modality Dimensions:\n",
      "   Expression: 60,660 features\n",
      "   Methylation: 220,147 features\n",
      "   Cnv: 56,756 features\n",
      "   Phenotype: 78 features\n",
      "\n",
      "üîÑ Preparing mixed integration data...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Prepare data\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müîÑ Preparing mixed integration data...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m mixed_data = \u001b[43mprepare_mixed_integration_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43momics_datasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müìä Data Split Summary:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Training samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(mixed_data[\u001b[33m'\u001b[39m\u001b[33my_train\u001b[39m\u001b[33m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mprepare_mixed_integration_data\u001b[39m\u001b[34m(omics_datasets, labels, test_size, device)\u001b[39m\n\u001b[32m     19\u001b[39m test_data = {}\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m modality, data \u001b[38;5;129;01min\u001b[39;00m omics_datasets.items():\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     train_data[modality] = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFloatTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m     23\u001b[39m     test_data[modality] = torch.FloatTensor(data[test_idx]).to(device)\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Labels\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool."
     ]
    }
   ],
   "source": [
    "# Mixed Integration Experimental Setup and Execution\n",
    "print(\"MIXED INTEGRATION EXPERIMENTAL SETUP\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Configuration\n",
    "num_classes = len(np.unique(labels.values))\n",
    "modality_dims = {\n",
    "    'expression': expression_data.shape[1],\n",
    "    'methylation': methylation_data.shape[1],\n",
    "    'cnv': cnv_data.shape[1],\n",
    "    'phenotype': phenotype_data.shape[1]\n",
    "}\n",
    "\n",
    "print(f\"üìä Experimental Configuration:\")\n",
    "print(f\"   Number of classes: {num_classes}\")\n",
    "print(f\"   Total samples: {len(labels)}\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(f\"\\nüìã Modality Dimensions:\")\n",
    "for modality, dim in modality_dims.items():\n",
    "    print(f\"   {modality.capitalize()}: {dim:,} features\")\n",
    "\n",
    "# Prepare data\n",
    "print(f\"\\nüîÑ Preparing mixed integration data...\")\n",
    "mixed_data = prepare_mixed_integration_data(omics_datasets, labels, device=device)\n",
    "\n",
    "print(f\"üìä Data Split Summary:\")\n",
    "print(f\"   Training samples: {len(mixed_data['y_train'])}\")\n",
    "print(f\"   Test samples: {len(mixed_data['y_test'])}\")\n",
    "\n",
    "# Define experimental configurations\n",
    "configs = {\n",
    "    'end_to_end_small': {\n",
    "        'modality_latent_dims': {\n",
    "            'expression': 32, 'methylation': 32, 'cnv': 16, 'phenotype': 8\n",
    "        },\n",
    "        'fusion_latent_dim': 32,\n",
    "        'num_classes': num_classes,\n",
    "        'epochs': 100,\n",
    "        'lr': 0.001,\n",
    "        'recon_weight': 0.5,\n",
    "        'fusion_weight': 0.3\n",
    "    },\n",
    "    'end_to_end_medium': {\n",
    "        'modality_latent_dims': {\n",
    "            'expression': 64, 'methylation': 64, 'cnv': 32, 'phenotype': 16\n",
    "        },\n",
    "        'fusion_latent_dim': 64,\n",
    "        'num_classes': num_classes,\n",
    "        'epochs': 100,\n",
    "        'lr': 0.001,\n",
    "        'recon_weight': 0.5,\n",
    "        'fusion_weight': 0.3\n",
    "    },\n",
    "    'stage_wise_small': {\n",
    "        'modality_latent_dims': {\n",
    "            'expression': 32, 'methylation': 32, 'cnv': 16, 'phenotype': 8\n",
    "        },\n",
    "        'fusion_latent_dim': 32,\n",
    "        'num_classes': num_classes,\n",
    "        'stage1_epochs': 100,\n",
    "        'stage2_epochs': 100,\n",
    "        'lr': 0.001\n",
    "    },\n",
    "    'stage_wise_medium': {\n",
    "        'modality_latent_dims': {\n",
    "            'expression': 64, 'methylation': 64, 'cnv': 32, 'phenotype': 16\n",
    "        },\n",
    "        'fusion_latent_dim': 64,\n",
    "        'num_classes': num_classes,\n",
    "        'stage1_epochs': 100,\n",
    "        'stage2_epochs': 100,\n",
    "        'lr': 0.001\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\nüß™ Experimental Configurations:\")\n",
    "for config_name in configs.keys():\n",
    "    print(f\"   - {config_name}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Ready to run mixed integration experiments!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd140bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Mixed Integration Experiments\n",
    "print(\"EXECUTING MIXED INTEGRATION EXPERIMENTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Store all results\n",
    "mixed_integration_results = {}\n",
    "\n",
    "# Run End-to-End experiments\n",
    "print(\"üöÄ Running End-to-End Mixed Integration Experiments...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for config_name in ['end_to_end_small', 'end_to_end_medium']:\n",
    "    print(f\"\\nüîÑ Running {config_name}...\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    config = configs[config_name]\n",
    "    \n",
    "    try:\n",
    "        result = train_mixed_integration_system(\n",
    "            mixed_data, modality_dims, config, device\n",
    "        )\n",
    "        mixed_integration_results[config_name] = result\n",
    "        \n",
    "        print(f\"‚úÖ {config_name} completed:\")\n",
    "        print(f\"   Train Accuracy: {result['train_accuracy']:.4f}\")\n",
    "        print(f\"   Test Accuracy: {result['test_accuracy']:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in {config_name}: {e}\")\n",
    "        mixed_integration_results[config_name] = {'error': str(e)}\n",
    "\n",
    "# Run Stage-wise experiments\n",
    "print(f\"\\nüéØ Running Stage-wise Mixed Integration Experiments...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for config_name in ['stage_wise_small', 'stage_wise_medium']:\n",
    "    print(f\"\\nüîÑ Running {config_name}...\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    config = configs[config_name]\n",
    "    \n",
    "    try:\n",
    "        result = train_stage_wise_mixed_integration(\n",
    "            mixed_data, modality_dims, config, device\n",
    "        )\n",
    "        mixed_integration_results[config_name] = result\n",
    "        \n",
    "        print(f\"‚úÖ {config_name} completed:\")\n",
    "        print(f\"   Train Accuracy: {result['train_accuracy']:.4f}\")\n",
    "        print(f\"   Test Accuracy: {result['test_accuracy']:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in {config_name}: {e}\")\n",
    "        mixed_integration_results[config_name] = {'error': str(e)}\n",
    "\n",
    "print(f\"\\nüéâ Mixed Integration Experiments Completed!\")\n",
    "print(f\"üìä Results stored for {len(mixed_integration_results)} configurations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfcb7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixed Integration Results Analysis and ML Evaluation\n",
    "print(\"MIXED INTEGRATION RESULTS ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Function to evaluate multiple ML classifiers on embeddings\n",
    "def evaluate_ml_models_mixed(train_embeddings, y_train, test_embeddings, y_test):\n",
    "    \"\"\"Evaluate ML models on mixed integration embeddings\"\"\"\n",
    "    models = {\n",
    "        'LogisticRegression': LogisticRegression(max_iter=500),\n",
    "        'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "        'SVM': SVC(probability=True, random_state=42),\n",
    "        'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "        'NaiveBayes': GaussianNB()\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for name, clf in models.items():\n",
    "        clf.fit(train_embeddings, y_train)\n",
    "        y_pred = clf.predict(test_embeddings)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "        f1_micro = f1_score(y_test, y_pred, average='micro')\n",
    "        precision = precision_score(y_test, y_pred, average='macro')\n",
    "        recall = recall_score(y_test, y_pred, average='macro')\n",
    "        \n",
    "        # AUC\n",
    "        try:\n",
    "            y_prob = clf.predict_proba(test_embeddings)\n",
    "            auc = roc_auc_score(y_test, y_prob, multi_class='ovr')\n",
    "        except:\n",
    "            auc = None\n",
    "            \n",
    "        results[name] = {\n",
    "            'accuracy': acc,\n",
    "            'f1_macro': f1_macro,\n",
    "            'f1_micro': f1_micro,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'auc': auc\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Analyze results and evaluate with ML models\n",
    "print(\"üîç Analyzing Mixed Integration Results...\")\n",
    "\n",
    "ml_evaluation_results = {}\n",
    "direct_classification_results = {}\n",
    "\n",
    "for config_name, result in mixed_integration_results.items():\n",
    "    if 'error' in result:\n",
    "        print(f\"‚ùå Skipping {config_name} due to error: {result['error']}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nüìä Analyzing {config_name}...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Direct classification results\n",
    "    direct_classification_results[config_name] = {\n",
    "        'train_accuracy': result['train_accuracy'],\n",
    "        'test_accuracy': result['test_accuracy']\n",
    "    }\n",
    "    \n",
    "    print(f\"üéØ Direct Classification Results:\")\n",
    "    print(f\"   Train Accuracy: {result['train_accuracy']:.4f}\")\n",
    "    print(f\"   Test Accuracy: {result['test_accuracy']:.4f}\")\n",
    "    \n",
    "    # ML evaluation on fusion embeddings\n",
    "    if 'train_embeddings' in result and 'test_embeddings' in result:\n",
    "        print(f\"\\nüîç Evaluating ML models on fusion embeddings...\")\n",
    "        \n",
    "        ml_results = evaluate_ml_models_mixed(\n",
    "            result['train_embeddings'], \n",
    "            mixed_data['y_train'].cpu().numpy(),\n",
    "            result['test_embeddings'], \n",
    "            mixed_data['y_test'].cpu().numpy()\n",
    "        )\n",
    "        \n",
    "        ml_evaluation_results[config_name] = ml_results\n",
    "        \n",
    "        # Find best ML model\n",
    "        best_ml = max(ml_results.items(), key=lambda x: x[1]['accuracy'])\n",
    "        print(f\"üèÜ Best ML Model: {best_ml[0]} (Accuracy: {best_ml[1]['accuracy']:.4f})\")\n",
    "        \n",
    "        # Display top 3 ML results\n",
    "        sorted_ml = sorted(ml_results.items(), key=lambda x: x[1]['accuracy'], reverse=True)\n",
    "        print(f\"üìã Top 3 ML Models on Fusion Embeddings:\")\n",
    "        for i, (model_name, metrics) in enumerate(sorted_ml[:3]):\n",
    "            print(f\"   {i+1}. {model_name}: Acc={metrics['accuracy']:.4f}, \"\n",
    "                  f\"F1={metrics['f1_macro']:.4f}\")\n",
    "\n",
    "# Overall results summary\n",
    "print(f\"\\nüèÜ OVERALL MIXED INTEGRATION RESULTS SUMMARY:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_data = []\n",
    "\n",
    "for config_name, result in mixed_integration_results.items():\n",
    "    if 'error' in result:\n",
    "        continue\n",
    "        \n",
    "    # Direct classification\n",
    "    comparison_data.append({\n",
    "        'Configuration': config_name,\n",
    "        'Method': 'Direct_Classification',\n",
    "        'Accuracy': result['test_accuracy'],\n",
    "        'F1_Macro': None  # Not calculated for direct\n",
    "    })\n",
    "    \n",
    "    # Best ML model on embeddings\n",
    "    if config_name in ml_evaluation_results:\n",
    "        best_ml = max(ml_evaluation_results[config_name].items(), \n",
    "                     key=lambda x: x[1]['accuracy'])\n",
    "        comparison_data.append({\n",
    "            'Configuration': config_name,\n",
    "            'Method': f'Embedding_{best_ml[0]}',\n",
    "            'Accuracy': best_ml[1]['accuracy'],\n",
    "            'F1_Macro': best_ml[1]['f1_macro']\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame and display\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "if not comparison_df.empty:\n",
    "    print(\"\\nüìã Performance Comparison Table:\")\n",
    "    print(comparison_df.round(4).to_string(index=False))\n",
    "    \n",
    "    # Find overall best\n",
    "    best_overall = comparison_df.loc[comparison_df['Accuracy'].idxmax()]\n",
    "    print(f\"\\nü•á Best Overall Performance:\")\n",
    "    print(f\"   Configuration: {best_overall['Configuration']}\")\n",
    "    print(f\"   Method: {best_overall['Method']}\")\n",
    "    print(f\"   Accuracy: {best_overall['Accuracy']:.4f}\")\n",
    "\n",
    "# Visualization\n",
    "print(f\"\\nüìà Creating Mixed Integration Visualizations...\")\n",
    "\n",
    "if len([r for r in mixed_integration_results.values() if 'error' not in r]) >= 2:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Direct Classification Comparison\n",
    "    configs = [k for k in direct_classification_results.keys()]\n",
    "    test_accs = [direct_classification_results[k]['test_accuracy'] for k in configs]\n",
    "    \n",
    "    axes[0,0].bar(configs, test_accs)\n",
    "    axes[0,0].set_title('Direct Classification Accuracy')\n",
    "    axes[0,0].set_ylabel('Test Accuracy')\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 2. Best ML Model Comparison\n",
    "    if not comparison_df.empty:\n",
    "        ml_data = comparison_df[comparison_df['Method'].str.contains('Embedding')]\n",
    "        if not ml_data.empty:\n",
    "            sns.barplot(data=ml_data, x='Configuration', y='Accuracy', ax=axes[0,1])\n",
    "            axes[0,1].set_title('Best ML Model on Embeddings')\n",
    "            axes[0,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 3. Method Comparison\n",
    "    if not comparison_df.empty:\n",
    "        sns.boxplot(data=comparison_df, x='Method', y='Accuracy', ax=axes[1,0])\n",
    "        axes[1,0].set_title('Method Performance Distribution')\n",
    "        axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 4. Configuration Comparison\n",
    "    if not comparison_df.empty:\n",
    "        config_means = comparison_df.groupby('Configuration')['Accuracy'].mean()\n",
    "        axes[1,1].bar(config_means.index, config_means.values)\n",
    "        axes[1,1].set_title('Average Performance by Configuration')\n",
    "        axes[1,1].set_ylabel('Mean Accuracy')\n",
    "        axes[1,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Store comprehensive results\n",
    "mixed_integration_comprehensive_results = {\n",
    "    'experiment_results': mixed_integration_results,\n",
    "    'ml_evaluation_results': ml_evaluation_results,\n",
    "    'direct_classification_results': direct_classification_results,\n",
    "    'comparison_df': comparison_df,\n",
    "    'configurations': configs,\n",
    "    'modality_dims': modality_dims\n",
    "}\n",
    "\n",
    "print(f\"\\nüíæ Comprehensive results stored in 'mixed_integration_comprehensive_results'\")\n",
    "print(f\"‚úÖ Mixed Integration Analysis Complete!\")\n",
    "print(f\"üéØ Tested {len(mixed_integration_results)} configurations with hierarchical fusion\")\n",
    "print(f\"üèóÔ∏è Architecture: Individual Omics AE ‚Üí Fusion AE ‚Üí Classification\")\n",
    "print(f\"üìä Best accuracy achieved: {comparison_df['Accuracy'].max():.4f}\" if not comparison_df.empty else \"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
